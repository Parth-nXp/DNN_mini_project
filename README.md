# Mini Projects

This repository is a collection of small DNN experiments created for learning and practice. Each experiment focuses on understanding a specific concept such as data loading, model training, evaluation, and experimentation with neural networks using simple datasets.

1. [Rice Classification](rice_type_classification.ipynb) – It demonstrates a complete **binary classification pipeline using PyTorch** on tabular data, covering **data preprocessing, custom Dataset and DataLoader creation**, and **neural network modeling**.It highlights hands-on usage of **model training loops, loss optimization (BCELoss + Adam), validation/testing workflows**, and **performance visualization** through accuracy and loss curves.

2. [Breast Canser Classification](breast_cancer_classification.ipynb) - This mini-project demonstrates an end-to-end binary classification pipeline on tabular medical data using PyTorch. It covers dataset acquisition from Kaggle, exploratory preprocessing, label encoding (malignant vs benign), feature normalization, and train/validation/test splitting. The project implements a custom PyTorch Dataset and DataLoader, followed by a fully connected neural network with multiple hidden layers and sigmoid output for probabilistic prediction. It highlights hands-on training loops with BCELoss and Adam optimizer, device-aware (CPU/GPU) execution, and systematic evaluation using accuracy on validation and test sets. Model performance is analyzed through loss and accuracy curves, providing clear insights into convergence behavior and generalization on real-world clinical data.

3. [Moons Dataset Classification](moons_binary_classification.ipynb) – This mini-project demonstrates a clean binary classification experiment on a synthetic non-linearly separable dataset (two-moons) generated using sklearn.datasets.make_moons. It covers creating a toy dataset, packaging it into a Pandas DataFrame, performing simple feature scaling/normalization, and building a custom PyTorch Dataset + DataLoader for mini-batch training. The model is a small MLP (fully connected network) with ReLU activations and a sigmoid output layer, trained using a standard training loop with BCELoss and Adam optimizer on CPU/GPU. The experiment emphasizes how multi-layer perceptrons learn non-linear decision boundaries, and visualizes training behavior through a loss vs epochs curve for quick convergence inspection.

4. [MLP Bottleneck Dropout](mlp_bottleneck_dropout.ipynb) - This mini-project builds an end-to-end binary classification pipeline in PyTorch using a synthetic tabular dataset generated with make_classification. It covers data splitting with train_test_split, wrapping NumPy arrays into TensorDataset, efficient batching through DataLoader, and training a simple feedforward neural network with two hidden layers, ReLU activations, and dropout for regularization. The model is trained on GPU/CPU using BCELoss with an Adam optimizer, while torchinfo.summary is used to inspect layer-wise shapes and parameter counts for clarity and debugging. Training and validation losses are tracked across epochs and visualized with a clean loss curve, making it a practical experiment for understanding tabular MLP training dynamics, overfitting behavior, and basic evaluation workflow in PyTorch.

5. [DNN Stoke](dnn_stroke.ipynb) - This mini-project builds a complete binary classification pipeline in PyTorch using the Kaggle Stroke Prediction dataset downloaded via opendatasets. It demonstrates practical tabular preprocessing steps such as imputing missing BMI values, dropping unused identifiers, converting categorical variables into numeric form using mapping and one-hot encoding, and casting the final feature matrix to float tensors for training. A lightweight MLP (19→64→32→1) with ReLU activations is trained on GPU/CPU using BCEWithLogitsLoss and Adam, while torchinfo.summary is used to verify tensor shapes and parameter counts. Training dynamics are tracked by logging epoch-wise loss and plotting a loss curve, making this a clean hands-on example of taking a real-world healthcare tabular dataset from raw CSV to a working neural baseline in PyTorch.

6. [Credit Card Fraud](credit_card_fraud.ipynb) - This mini-project builds a practical fraud-detection pipeline in PyTorch using the Kaggle Credit Card Fraud dataset, focusing on real-world challenges of highly imbalanced binary classification. It covers loading and preparing tabular features, standardizing inputs with StandardScaler, splitting into train/test sets, and training a regularized MLP with batch normalization and dropout for stable optimization on large batches. To address imbalance, it uses BCEWithLogitsLoss with a computed pos_weight, and trains with AdamW including weight decay for better generalization. The notebook uses torchinfo.summary to validate model shapes and parameter counts, and tracks both loss and recall over epochs (with thresholded sigmoid outputs) to emphasize minority-class detection performance, visualizing train vs test trends for both metrics to understand overfitting and detection quality.

7. 
